{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zV_LiEbybwjM",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 简介\n",
    "\n",
    "本次作业包含2个代码填空和5个Exercise。\n",
    "\n",
    "回顾课堂内容，我们知道TRPO在很多场景上都很成功，但是我们也发现了它的计算过程非常的复杂，每步更新的运算量非常大。于是，在2017年TRPO的改进版PPO算法被提出，它基于TRPO的思想，但是实现算法更加简单，避免了复杂的求KL散度的Hessian矩阵。并且大量的实验结果表明，PPO能够比TRPO学习的更快，这使得PPO一下子成为了非常流行的强化学习算法。如果我们想要尝试在一个新的环境用强化学习，那么PPO就属于那种可以首先尝试的算法。\n",
    "\n",
    "## PPO算法\n",
    "\n",
    "我们回忆一下TRPO的优化目标：\n",
    "$$\n",
    " \\max\\limits_{\\theta}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]\\quad\\text{s.t.}\\quad \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s))\\right]\\le\\delta\n",
    "$$\n",
    "\n",
    "TRPO使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO的优化目标同样是它，但PPO用了一些相对简单的方法来求解。具体来说，PPO有两种形式，一是PPO-Penalty，二是PPO-Clip。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWW0yZsuc9kv",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### PPO-Penalty\n",
    "\n",
    "PPO-Penalty用Lagrange乘子法直接将KL散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，然后在迭代的过程中不断更新KL散度前的系数。即\n",
    "$$\n",
    "\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s)]\\right]\n",
    "$$\n",
    "\n",
    "令 $d_k=D_{KL}^{\\nu^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_\\theta)$，$\\beta$ 的更新规则如下：\n",
    "1. 如果$d_k<\\delta/1.5$，那么$\\beta_{k+1}\\leftarrow \\beta_k/2$\n",
    "2. 如果$d_k>\\delta\\times1.5$，那么$\\beta_{k+1}\\leftarrow \\beta_k\\times 2$\n",
    "3. 否则$\\beta_{k+1}=\\beta_k$。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5e0rjldxgkFA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### PPO-Clip\n",
    "\n",
    "PPO-Clip更加直接，其直接在目标函数里进行限制，以保证新的参数和旧的参数的差距不会太大，即\n",
    "\n",
    "$$\n",
    "\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a),\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon\\right)A^{\\pi_{\\theta_k}}(s,a)\\right)\\right]\n",
    "$$\n",
    "\n",
    "其中 $\\text{clip}(x,l,r):=\\max(\\min(x,r),l)$ ，即把 $x$ 限制在 $[l,r]$ 内。上式中$\\epsilon$是一个超参数，表示clip的范围。如果$A(s,a)>0$，说明这个动作的Q值高于平均，最大化这个式子会增大$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1+\\epsilon$。反之，如果$A(s,a)<0$，最大化这个式子会减小$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1-\\epsilon$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PPO 算法改进\n",
    "在标准的 PPO-Clip 算法里，我们使用 Clip 来约束新策略和旧策略之间的差异。新推出的 GRPO 算法对 PPO-Clip 做了一些改进，我们在这里尝试其中的两项改进：\n",
    "1、结合 PPO-Penalty，将 KL 散度的惩罚项添加到目标函数中；\n",
    "2、使用KL 散度的无偏估计替代对KL散度的直接计算\n",
    "新的更新目标如下：\n",
    "$$\n",
    "\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a),\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon\\right)A^{\\pi_{\\theta_k}}(s,a)\\right)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s)]\\right].\n",
    "$$\n",
    "\n",
    "其中 KL散度采用如下链接中给出的无偏估计 http://joschu.net/blog/kl-approx.html :\n",
    "$$\n",
    "D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s)]=\\frac{\\pi_{\\theta}(\\cdot|s)}{\\pi_{\\theta_k}(\\cdot|s)}-\\log\\frac{\\pi_{\\theta}(\\cdot|s)}{\\pi_{\\theta_k}(\\cdot|s)}-1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 1**. 结合阅读材料 http://joschu.net/blog/kl-approx.html，说明为何要取如下的 estimator:\n",
    "对于样本 $x_1,x_2\\cdots ~ q$，令 $r=\\frac{p(x)}{q(x)}$，我们有\n",
    "$$\n",
    "D_{KL}[p,q]=r\\log r-(r-1)\\\\\n",
    "D_{KL}[q,p]=(r-1)-\\log r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**解答**：\n",
    "\n",
    "在 PPO 算法中，我们希望限制新策略与旧策略的差异，从而保持稳定性,因此这样取使用KL散度来度量策略变化的程度，可以作为一种正则项或指标来控制更新步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsYckcOoDOUW",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# PPO代码实践\n",
    "\n",
    "我们在两个环境CartPle和Pendulum上测试PPO算法。我们采用改进后的PPO算法，并且通过参数搜索寻找最合适的$\\beta$值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EftJCiSI-DOk",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 环境1\n",
    "\n",
    "首先导入一些必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % pip install gym\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0-WdD7cQhd8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们的PPOAgent主要有两个网络：policy网络和value网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGQDcsxLQ2D0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"创建两种agent，并设定部分超参数。\"\"\"\n",
    "\n",
    "    def __init__(self, feature_n, action_n):\n",
    "        self.policy_model = torch.nn.Sequential(torch.nn.Linear(feature_n, 30),\n",
    "                                                torch.nn.Tanh(),\n",
    "                                                torch.nn.Linear(30, action_n),\n",
    "                                                torch.nn.Softmax(dim=1))\n",
    "        self.value_model = torch.nn.Sequential(torch.nn.Linear(feature_n, 30),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(30, 10),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(10, 1))\n",
    "\n",
    "        self.policy_optim = torch.optim.Adam(self.policy_model.parameters(), lr=1e-4)\n",
    "        self.value_optim = torch.optim.Adam(self.value_model.parameters(), lr=3e-4)\n",
    "\n",
    "        self.gamma = 0.98  # 折扣因子\n",
    "        self.batch_size = 128\n",
    "        self.eps = 0.2\n",
    "\n",
    "    \"\"\"根据给定的状态，采样动作。\"\"\"\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "        prob = self.policy_model(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs=prob)\n",
    "        return dist.sample(), prob\n",
    "\n",
    "    \"\"\"根据给定的状态，计算V函数。\"\"\"\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return self.value_model(state)\n",
    "\n",
    "    \"\"\"策略学习。\"\"\"\n",
    "\n",
    "    def policy_learn(self, s, a, old_pro, adv, beta=0.1):\n",
    "        ########################################\n",
    "        ## Programming 1:策略更新\n",
    "        ########################################\n",
    "        new_probs = self.policy_model(s)\n",
    "        new_probs_a = new_probs.gather(1, a) \n",
    "        old_probs_a = old_pro.gather(1, a) \n",
    "\n",
    "        prob_ratio = (new_probs_a / (old_probs_a.detach() + 1e-8)) \n",
    "\n",
    "        surr1 = prob_ratio * adv \n",
    "        surr2 = torch.clamp(prob_ratio, 1 - self.eps, 1 + self.eps) * adv  \n",
    "        policy_loss = -torch.min(surr1, surr2).mean() \n",
    "\n",
    "        entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-8), dim=1).mean() \n",
    "        policy_loss -= beta * entropy \n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 0.5) \n",
    "        self.policy_optim.step()\n",
    "        #############################\n",
    "        ########################################\n",
    "        ## End of Programming 1\n",
    "        ########################################\n",
    "        return prob_ratio  # 返回probability ratio以观察训练过程\n",
    "\n",
    "    \"\"\"价值学习。\"\"\"\n",
    "\n",
    "    def value_learn(self, s, r, d, s_):\n",
    "        v_ = self.get_value(s_)\n",
    "        v = self.get_value(s)\n",
    "        td_error = v - (r + (1 - d) * self.gamma * v_).detach()\n",
    "        loss = td_error.pow(2).mean()\n",
    "\n",
    "        self.value_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), .5)\n",
    "        self.value_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvYSOuYVCMW8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "计算advantage函数的时候，对于Q函数的计算做一步展开（TD）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z9eSRUa6r-y",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_advantage(s, s_, r, d, agent):\n",
    "    with torch.no_grad():\n",
    "        q_fn = r + (1 - d) * agent.gamma * agent.value_model(s_)\n",
    "    return q_fn - agent.value_model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7lT6DolRpvg",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "创建环境，并设定随机数种子以便重复实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "seed = 999\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2DQG4Z6Rpvm",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "实例化agent并开始训练。在训练过程中，我们会动态描绘训练曲线（横坐标是episode，纵坐标是对应的reward）。\n",
    "\n",
    "当训练出的agent已经足够好时，训练停止并输出“Solved!”。\n",
    "\n",
    "期望运行时间：$1$分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = 200\n",
    "line = 500\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "epi_n = 1500\n",
    "mini_epoch = 15\n",
    "i_list = []\n",
    "r_list = []\n",
    "p_list = []\n",
    "p_i_list = []\n",
    "\n",
    "s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epi_n + 1):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    tot_reward = 0\n",
    "    while not done:\n",
    "        action, action_distribution = agent.sample_action([state])\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        tot_reward += reward\n",
    "\n",
    "        if done:\n",
    "            reward = -20\n",
    "\n",
    "        s.append(state)\n",
    "        a.append(action)\n",
    "        p.append(action_distribution)\n",
    "        r.append(reward)\n",
    "        s_.append(next_state)\n",
    "        d.append(done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if len(s) >= 200:\n",
    "        # print(f\"collect {len(s)} samples\")\n",
    "        s = torch.tensor(s, dtype=torch.float)\n",
    "        a = torch.tensor(a, dtype=torch.long).view(-1, 1)\n",
    "        p = torch.cat(p).view(-1, 2)\n",
    "        r = torch.tensor(r, dtype=torch.float).view(-1, 1)\n",
    "        s_ = torch.tensor(s_, dtype=torch.float)\n",
    "        d = torch.tensor(d, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        adv = compute_advantage(s, s_, r, d, agent).detach().view(-1, 1)\n",
    "\n",
    "        prob_ratio = 0\n",
    "        for _ in range(mini_epoch):\n",
    "            prob_list = []\n",
    "            for idx in torch.utils.data.sampler.BatchSampler(\n",
    "                    torch.utils.data.sampler.SubsetRandomSampler(range(len(s))), agent.batch_size, False):\n",
    "                prob = agent.policy_learn(s[idx], a[idx], p[idx], adv[idx])\n",
    "                agent.value_learn(s[idx], r[idx], d[idx], s_[idx])\n",
    "                prob_list.append(prob.mean().item())\n",
    "            prob_ratio += np.mean(prob_list)\n",
    "        p_list.append(prob_ratio / mini_epoch)\n",
    "        p_i_list.append(i)\n",
    "        # on-policy 训练\n",
    "        s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "    # 画图\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot([0, i], [line, line])\n",
    "        i_list.append(i)\n",
    "        tot_reward_list = []\n",
    "        for _ in range(5):\n",
    "            tot_reward = 0\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            while not done:\n",
    "                _, prob = agent.sample_action([state])\n",
    "                action = prob.argmax(-1)\n",
    "                state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                done = terminated or truncated\n",
    "                tot_reward += reward\n",
    "            tot_reward_list.append(tot_reward)\n",
    "        r_list.append(np.mean(tot_reward_list))\n",
    "        plt.plot(i_list, r_list)\n",
    "        clear_output(True)\n",
    "        plt.xlabel('episodes')\n",
    "        plt.ylabel('accumulated reward')\n",
    "        plt.figure()\n",
    "        plt.plot(p_i_list, p_list)\n",
    "        plt.xlabel('episodes')\n",
    "        plt.ylabel('prob ratio')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A30741B84E57449897C117A343762ADE",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "609a3a8206b94200178e6452",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "参考训练过程如下：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/rt/w39tuKE-Rwv_/qsxj2k8ece.png)\n",
    "\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/rt/w39tuKE-Rwv_/qsxj2kvekg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在上面的图中，probability ratio的值可以用来作为mini-epoch，learning rate这些参数调节的依据。离散动作空间中，在训练中期，该值可以说明策略更新的幅度，且应该保持在较大的值。而在下面的连续动作空间中，由于假设习得的动作分布是高斯分布，该值一般较为稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1472BFC9EF764DDD83B2CE0F05D24D0F",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60980a9506b94200178d55c4",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "改进的ppo算法存在超参数 $\\beta$, 我们可以通过调整这个参数来观察训练过程中的变化。请结合先前代码，在下面的代码块中设计一个简单的参数搜索算法，来寻找最优的 $\\beta$ 值。并在Exercise 2中简要说明你的参数搜索算法是如何评估 $\\beta$ 值的好坏的并给出你认为的最优的 $\\beta$ 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_agent_with_beta(beta_value, env, num_episodes=500, mini_epoch=5):\n",
    "    feature_n = env.observation_space.shape[0]\n",
    "    action_n = env.action_space.n\n",
    "\n",
    "    agent = PPOAgent(feature_n, action_n)\n",
    "    cumulative_rewards = [] \n",
    "\n",
    "    s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # 解包返回两个值\n",
    "            action, action_log_prob = agent.sample_action([state])\n",
    "            action = int(action)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "            s.append(state)\n",
    "            a.append(action)\n",
    "            p.append(action_log_prob)\n",
    "            r.append(reward)\n",
    "            s_.append(next_state)\n",
    "            d.append(done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        cumulative_rewards.append(episode_reward)\n",
    "\n",
    "        if len(s) >= 1000:\n",
    "            s_tensor   = torch.tensor(s, dtype=torch.float)\n",
    "            a_tensor   = torch.tensor(a, dtype=torch.long).view(-1, 1)\n",
    "            p_tensor   = torch.cat(p, dim=0)\n",
    "            r_tensor   = torch.tensor(r, dtype=torch.float).view(-1, 1)\n",
    "            s_tensor_  = torch.tensor(s_, dtype=torch.float)\n",
    "            d_tensor   = torch.tensor(d, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "            # 奖励归一化（可选）\n",
    "            r_tensor = (r_tensor - r_tensor.mean()) / (r_tensor.std() + 1e-8)\n",
    "            adv = compute_advantage(s_tensor, s_tensor_, r_tensor, d_tensor, agent).detach().view(-1, 1)\n",
    "\n",
    "            for _ in range(mini_epoch):\n",
    "                for idx in torch.utils.data.BatchSampler(\n",
    "                        torch.utils.data.SubsetRandomSampler(range(len(s_tensor))),\n",
    "                        agent.batch_size,\n",
    "                        drop_last=False):\n",
    "                    agent.policy_learn(s_tensor[idx], a_tensor[idx], p_tensor[idx], adv[idx], beta=beta_value)\n",
    "                    agent.value_learn(s_tensor[idx], r_tensor[idx], d_tensor[idx], s_tensor_[idx])\n",
    "            s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "    avg_final_reward = np.mean(cumulative_rewards[-10:])\n",
    "    return avg_final_reward\n",
    "\n",
    "\n",
    "# 调用部分\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "beta_candidates = np.arange(0.001, 0.1001, 0.005)\n",
    "beta_results = {}\n",
    "\n",
    "print(\"开始进行 β 参数的网格搜索（Exercise 2）……\")\n",
    "for beta in beta_candidates:\n",
    "    avg_reward = train_agent_with_beta(beta_value=beta, env=env, num_episodes=500, mini_epoch=5)\n",
    "    beta_results[beta] = avg_reward\n",
    "    print(f\"Beta: {beta:.4f}  --> 最后 10 个 episode 平均累计奖励: {avg_reward:.2f}\")\n",
    "\n",
    "best_beta = max(beta_results, key=beta_results.get)\n",
    "print(\"\\n参数搜索完成。\")\n",
    "print(f\"最优的 β 值为：{best_beta:.4f}，对应的平均累计奖励为：{beta_results[best_beta]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 2**. 请在此简要说明你的参数搜索算法是如何评估 $\\beta$ 值的好坏的并给出你认为的最优的 $\\beta$ 值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 采用网格搜索方法优化超参数 beta，其搜索范围设定为 [0.001, 0.1]，步长为 0.005，共进行了约 20 次实验。每次实验，重新初始化 PPOAgent，并训练固定数量（500 个）的 episode，记录每个 episode 的累计奖励，最终以最后 10 个episode 的平均累计奖励作为性能评估指标。\n",
    "\n",
    "- 对于每个 beta 值，累计奖励越高，说明该 beta 值使策略收敛更快、表现更好，因此被认为更优。\n",
    "\n",
    "- 最优的beta值为：0.036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI-XslbS3vep",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 连续动作环境\n",
    "\n",
    "对于连续动作环境，我们需要对上面的代码做一定的修改。具体来说，需要修改以下代码块（直接运行即可）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmpa0Z2u-GbM",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "下面是新的policy网络，因为环境是连续动作的，因此我们的网络分别输出表示动作分布的高斯分布的 $\\mu$ 和 $\\sigma$ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE-fFxkrCZMj",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyModel(torch.nn.Module):\n",
    "    def __init__(self, feature_n):\n",
    "        super(PolicyModel, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(feature_n, 100)\n",
    "        self.l2 = torch.nn.Linear(100, 1)\n",
    "        self.l3 = torch.nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.l1(x))\n",
    "        mu = 2 * torch.tanh(self.l2(x))\n",
    "        sigma = torch.nn.functional.softplus(self.l3(x))\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4964BJCdQmZD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以及修改一些主函数。\n",
    "\n",
    "这和上面的PPOAgent是类似的，只有部分修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfX-xwZjAO9i",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "\n",
    "    def __init__(self, feature_n, action_n):\n",
    "        self.policy_model = PolicyModel(feature_n)\n",
    "        self.value_model = torch.nn.Sequential(torch.nn.Linear(feature_n, 100),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(100, 1))\n",
    "\n",
    "        self.policy_optim = torch.optim.Adam(self.policy_model.parameters(), lr=1e-4)\n",
    "        self.value_optim = torch.optim.Adam(self.value_model.parameters(), lr=3e-4)\n",
    "\n",
    "        self.gamma = 0.9  # 折扣因子\n",
    "        self.batch_size = 128\n",
    "        self.eps = 0.2\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = self.policy_model(state_tensor)\n",
    "        dist = torch.distributions.normal.Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action)\n",
    "        return action.item(), action_log_prob.item(), dist\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return self.value_model(state)\n",
    "\n",
    "    def policy_learn(self, s, a, old_p, adv, beta = 0.05):\n",
    "        ########################################\n",
    "        ## Programming 2:策略更新\n",
    "        ########################################\n",
    "        mu, sigma = self.policy_model(s)\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "\n",
    "        new_log_prob = dist.log_prob(a) \n",
    "\n",
    "        ratio = torch.exp(new_log_prob - old_p) \n",
    "   \n",
    "        surr1 = ratio * adv\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * adv  \n",
    "        policy_loss = -torch.min(surr1, surr2).mean() \n",
    "        \n",
    "        entropy = dist.entropy().mean() \n",
    "        policy_loss -= beta * entropy \n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 0.5)  \n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        prob_ratio = ratio.mean() \n",
    "        ########################################\n",
    "        ## End of Programming 2\n",
    "        ########################################\n",
    "        return prob_ratio\n",
    "\n",
    "    def value_learn(self, s, r, d, s_):\n",
    "        v_ = self.get_value(s_)\n",
    "        v = self.get_value(s)\n",
    "        td_error = v - (r + (1 - d) * self.gamma * v_).detach()\n",
    "        loss = td_error.pow(2).mean()\n",
    "\n",
    "        self.value_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), .5)\n",
    "        self.value_optim.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-X3mRwuCaHG",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "创建环境Pendulum-v1，并设定随机数种子以便重复实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "seed = 0\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwSQw7s8_Vry",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "实例化agent并开始训练。在训练过程中，我们会动态描绘训练曲线（横坐标是episode，纵坐标是对应的reward）。\n",
    "\n",
    "当训练出的agent已经足够好时，训练停止并输出“Solved!”。\n",
    "\n",
    "期望运行时间：$3$分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = -200\n",
    "\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "epi_n = 1500\n",
    "mini_epoch = 15\n",
    "i_list = []\n",
    "r_list = []\n",
    "p_list = []\n",
    "p_i_list = []\n",
    "s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epi_n + 1):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    tot_reward = 0\n",
    "    while not done:\n",
    "        action, action_distribution, _ = agent.sample_action([state])\n",
    "        next_state, reward, terminated, truncated, info = env.step([action])\n",
    "        done = terminated or truncated\n",
    "        tot_reward += reward\n",
    "\n",
    "        s.append(state)\n",
    "        a.append(action)\n",
    "        p.append(action_distribution)\n",
    "        r.append(reward)\n",
    "        s_.append(next_state)\n",
    "        d.append(done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if len(s) >= 1000:\n",
    "        s = torch.tensor(s, dtype=torch.float)\n",
    "        a = torch.tensor(a, dtype=torch.float).view(-1, 1)\n",
    "        p = torch.tensor(p, dtype=torch.float).view(-1, 1)\n",
    "        r = torch.tensor(r, dtype=torch.float).view(-1, 1)\n",
    "        s_ = torch.tensor(s_, dtype=torch.float)\n",
    "        d = torch.tensor(d, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        r = (r - r.mean()) / (r.std() + 1e-8)\n",
    "\n",
    "        adv = compute_advantage(s, s_, r, d, agent).detach().view(-1, 1)\n",
    "\n",
    "        prob_ratio = 0\n",
    "        for _ in range(mini_epoch):\n",
    "            prob_list = []\n",
    "            for idx in torch.utils.data.sampler.BatchSampler(\n",
    "                    torch.utils.data.sampler.SubsetRandomSampler(range(len(s))), agent.batch_size, False):\n",
    "                prob = agent.policy_learn(s[idx], a[idx], p[idx], adv[idx])\n",
    "                agent.value_learn(s[idx], r[idx], d[idx], s_[idx])\n",
    "                prob_list.append(prob.mean().item())\n",
    "            prob_ratio += np.mean(prob_list)\n",
    "        p_list.append(prob_ratio / mini_epoch)\n",
    "        p_i_list.append(i)\n",
    "        # on-policy 训练\n",
    "        s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "    # 画图\n",
    "    if i % 100 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot([0, i], [line, line])\n",
    "        i_list.append(i)\n",
    "        tot_reward_list = []\n",
    "        for _ in range(5):\n",
    "            tot_reward = 0\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            while not done:\n",
    "                _, _, prob = agent.sample_action([state])\n",
    "                action = prob.mean\n",
    "                state, reward, terminated, truncated, _ = env.step([action.item()])\n",
    "                done = terminated or truncated\n",
    "                tot_reward += reward\n",
    "            tot_reward_list.append(tot_reward)\n",
    "        r_list.append(np.mean(tot_reward_list))\n",
    "        plt.plot(i_list, r_list)\n",
    "        clear_output(True)\n",
    "        plt.xlabel('episodes')\n",
    "        plt.ylabel('accumulated reward')\n",
    "        plt.figure()\n",
    "        plt.plot(p_i_list, p_list)\n",
    "        plt.xlabel('episodes')\n",
    "        plt.ylabel('prob ratio')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBA79A918BA44B07868766BD3505F634",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "609a3a8206b94200178e6452",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "参考训练过程如下：\n",
    "\n",
    "![Image Name](images/1.png)\n",
    "\n",
    "![Image Name](images/2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "同样的，请在此实现对 $beta$ 的参数搜索算法，并在Exercise 3中简要说明你的参数搜索算法是如何评估 $\\beta$ 值的好坏的并给出你认为的最优的 $\\beta$ 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_agent_with_beta(beta_value, env, num_episodes=500, mini_epoch=5):\n",
    "    feature_n = env.observation_space.shape[0]\n",
    "    action_n = env.action_space.shape[0]\n",
    "    agent = PPOAgent(feature_n, action_n)\n",
    "    \n",
    "    cumulative_rewards = []\n",
    "    s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, action_log_prob, _ = agent.sample_action([state])\n",
    "            next_state, reward, terminated, truncated, info = env.step([action])\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            s.append(state)\n",
    "            a.append(action)\n",
    "            p.append(action_log_prob)\n",
    "            r.append(reward)\n",
    "            s_.append(next_state)\n",
    "            d.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        cumulative_rewards.append(episode_reward)\n",
    "\n",
    "        if len(s) >= 1000:\n",
    "            s_tensor = torch.tensor(s, dtype=torch.float)\n",
    "            a_tensor = torch.tensor(a, dtype=torch.float).view(-1, 1)\n",
    "            p_tensor = torch.tensor(p, dtype=torch.float).view(-1, 1)\n",
    "            r_tensor = torch.tensor(r, dtype=torch.float).view(-1, 1)\n",
    "            s_tensor_ = torch.tensor(s_, dtype=torch.float)\n",
    "            d_tensor = torch.tensor(d, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "            r_tensor = (r_tensor - r_tensor.mean()) / (r_tensor.std() + 1e-8)\n",
    "            \n",
    "            adv = compute_advantage(s_tensor, s_tensor_, r_tensor, d_tensor, agent).detach().view(-1, 1)\n",
    "            \n",
    "            for _ in range(mini_epoch):\n",
    "                for idx in torch.utils.data.BatchSampler(\n",
    "                        torch.utils.data.SubsetRandomSampler(range(len(s_tensor))),\n",
    "                        agent.batch_size,\n",
    "                        drop_last=False):\n",
    "                    agent.policy_learn(s_tensor[idx], a_tensor[idx], p_tensor[idx], adv[idx], beta=beta_value)\n",
    "                    agent.value_learn(s_tensor[idx], r_tensor[idx], d_tensor[idx], s_tensor_[idx])\n",
    "\n",
    "            s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "  \n",
    "    avg_final_reward = np.mean(cumulative_rewards[-10:])\n",
    "    return avg_final_reward\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\") \n",
    "\n",
    "beta_candidates = np.arange(0.001, 0.1001, 0.005) \n",
    "beta_results = {}\n",
    "\n",
    "print(\"开始对 beta 参数进行搜索……\")\n",
    "for beta in beta_candidates:\n",
    "    avg_reward = train_agent_with_beta(beta_value=beta, env=env, num_episodes=500, mini_epoch=5)\n",
    "    beta_results[beta] = avg_reward\n",
    "    print(f\"Beta: {beta:.4f}, 最后10个 episode 平均累计奖励: {avg_reward:.2f}\")\n",
    "\n",
    "best_beta = max(beta_results, key=beta_results.get)\n",
    "print(\"\\n参数搜索完成。\")\n",
    "print(f\"最优的 beta 值为：{best_beta:.4f}，对应的平均累计奖励为：{beta_results[best_beta]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 3**. 请在此简要说明你的参数搜索算法是如何评估 $\\beta$ 值的好坏的并给出你认为的最优的 $\\beta$ 值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "回答：\n",
    "\n",
    "- 采用网格搜索方法优化超参数 beta，其搜索范围设定为 [0.001, 0.1]，步长为 0.005，共进行了约 20 次实验。每次实验，重新初始化 PPOAgent，并训练固定数量（500 个）的 episode，记录每个 episode 的累计奖励，最终以最后 10 个episode 的平均累计奖励作为性能评估指标。\n",
    "\n",
    "- 对于每个 beta 值，累计奖励越高，说明该 beta 值使策略收敛更快、表现更好，因此被认为更优。\n",
    "\n",
    "- 最优的beta值为：0.0110\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E3B941164DD4693AF1D314270F5F1E0",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60980a9506b94200178d55c4",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 更新目标理论\n",
    "学习完这两章，现在我们给出一些关于策略梯度算法的更新目标的理论：\n",
    "首先，由策略梯度定理（证明见策略梯度章节的作业），我们知道：\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] = \n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)\\left(\\sum_{t^{\\prime}=t}^{T-1} r_{t^{\\prime}}\\right)\\right] \n",
    "\\tag{1}\n",
    "$$\n",
    "对比PPO的更新目标（这里使用GAE($\\lambda,0)$）$\\mathbb{E}_{s_t,s_{t+1} \\sim \\pi_{\\theta}}[r_t+\\gamma V(s_{t+1})-V(s_{t})]$和公式(1)，发现其中的差异主要在于减去了一项$V(s_{t})$，这一项通常被称为Baseline，此项一般只与状态有关。\n",
    "\n",
    "将Baseline表示为$b(s_t)$，引入Baseline，有\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] = \n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)\\left(\\sum_{t^{\\prime}=t}^{T-1} r_{t^{\\prime}}-b(s_t)\\right)-\\right] \n",
    "\\tag{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3AC3CD6C4934D4B8201B37BCF2DD41F",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60980a9506b94200178d55c4",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Exercise 4**. 证明\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) b\\left(s_{t}\\right)\\right] = 0 \\tag{3}.\n",
    "$$\n",
    "\n",
    "提示1：$\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) b\\left(s_{t}\\right)\\right]=\\mathbb{E}_{s_{0: t}, a_{0: t-1}}\\left[\\mathbb{E}_{s_{t+1: T}, a_{t: T-1}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) b\\left(s_{t}\\right)\\right]\\right]$.\n",
    "\n",
    "提示2：要**证明** $\\mathbb{E}_{a_{t}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)\\right]=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**证明**：\n",
    "\n",
    "对于给定状态$s_t$，考虑动作的分布$\\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)$。根据概率密度函数的性质，有\n",
    "\n",
    "$$\\int \\pi_{\\theta}\\left(a \\mid s_{t}\\right) d a=1$$\n",
    "\n",
    "对参数 $\\theta$ 求梯度，两边得到\n",
    "\n",
    "$$\\nabla_{\\theta} \\int \\pi_\\theta(a \\mid s_t) \\, da = \\nabla_{\\theta} 1 = 0$$\n",
    "\n",
    "化简得：\n",
    "\n",
    "$$\\int \\nabla_{\\theta} \\pi_\\theta(a \\mid s_t) \\, da = 0$$\n",
    "\n",
    "又有：\n",
    "\n",
    "$$\\nabla_{\\theta} \\pi_\\theta(a \\mid s_t) = \\pi_\\theta(a \\mid s_t) \\, \\nabla_{\\theta} \\log \\pi_\\theta(a \\mid s_t)$$\n",
    "\n",
    "带入得：\n",
    "\n",
    "$$\\int \\pi_\\theta(a \\mid s_t) \\, \\nabla_{\\theta} \\log \\pi_\\theta(a \\mid s_t) \\, da = 0$$\n",
    "\n",
    "即：\n",
    "\n",
    "$$\\mathbb{E}_{a_{t}\\sim \\pi_\\theta(\\cdot \\mid s_t)}\\left[\\nabla_{\\theta} \\log \\pi_\\theta(a_t \\mid s_t)\\right] = 0.$$\n",
    "\n",
    "现在，考虑包含 $b(s_t)$ 的期望，由于 $b(s_t) 与 a_t$ 无关，有\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{a_{t}\\sim \\pi_\\theta(\\cdot \\mid s_t)}\\left[\\nabla_{\\theta} \\log \\pi_\\theta(a_t \\mid s_t)b(s_t)\\right] = b(s_t) \\, \\mathbb{E}_{a_{t}\\sim \\pi_\\theta(\\cdot \\mid s_t)}\\left[\\nabla_{\\theta} \\log \\pi_\\theta(a_t \\mid s_t)\\right]= b(s_t) \\cdot 0 = 0\n",
    "$$\n",
    "\n",
    "利用全概率公式，对整个轨迹 $\\tau \\sim \\pi_\\theta$ 做期望，可以写成\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\Bigl[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t\\mid s_t)b(s_t)\\Bigr] = \\mathbb{E}_{s_{0:t},\\, a_{0:t-1}}\\left[\\mathbb{E}_{s_{t+1:T},\\, a_{t:T-1}}\\Bigl[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t\\mid s_t)\\, b(s_t)\\Bigr]\\right] = \\mathbb{E}_{s_t}\\left[b(s_t)\\, \\mathbb{E}_{a_t\\sim \\pi_{\\theta}(\\cdot \\mid s_t)}\\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t\\mid s_t)\\right]\\right] = \\mathbb{E}_{s_t}\\left[b(s_t) \\cdot 0\\right] = 0\n",
    "$$\n",
    "\n",
    "得证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**.\n",
    "证明\n",
    "1. 求出最小化$Var(\\widehat{\\nabla_{\\theta} J})$的$b^{*}(s^{k}_t)$, 并指出其在何种条件下等于$\\mathbb{E}_{\\tau}\\left[\\sum_{t^{\\prime}=t}^{T-1} r^{k}_{t^{\\prime}}(s_{t^{\\prime}}^k, a_{t^{\\prime}}^k)\\right]=V(s^k_t)$.\n",
    "2. 记不用Baseline的梯度估计为\n",
    "$$\n",
    "\\widehat{\\nabla_{\\theta} J}^{\\prime} = \\frac{1}{N}\\sum_{k=0}^{N} \\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t}^{k} \\mid s_{t}^{k}\\right)\\left(\\sum_{t^{\\prime}=t}^{T-1} r^{k}_{t^{\\prime}}\\right)\\right]. \\tag{5}\n",
    "$$\n",
    "证明使用$b^{*}$的梯度估计方差小于该不用baseline的梯度估计方差。\n",
    "\n",
    "提示1：$Var(X)=\\mathbb{E}(X^2)-(\\mathbb{E}(X))^2$, $Var(X+Y) = Var(X)+Var(Y)+2Cov(X,Y)$\n",
    "\n",
    "提示2：使用**Ex 1**的结论。\n",
    "\n",
    "提示3： 最小二乘法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**证明**：\n",
    "\n",
    "1.\n",
    "\n",
    "我们考虑带 baseline 的策略梯度估计，其形式为:\n",
    "\n",
    "$$\\widehat{\\nabla_{\\theta} J}(b) = \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\bigl(a_{t} \\mid s_{t}\\bigr) \\left( \\sum_{t'=t}^{T-1} r_{t'} - b(s_t) \\right)$$\n",
    "\n",
    "又，我们知道对于任意与动作无关的 baseline $b(s_t)$ 都不改变梯度估计的无偏性，而可以用来降低方差。\n",
    "\n",
    "令$G_t = \\sum_{t'=t}^{T-1} r_{t'} \\quad $和$ \\quad X = \\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}\\mid s_t)$\n",
    "\n",
    "考虑在给定状态 \\(s_t\\) 下的局部目标函数:\n",
    "\n",
    "$\\mathcal{V}(b) = \\mathbb{E}_{a_t\\sim\\pi_\\theta(\\cdot\\mid s_t)}\\Bigl[\\bigl\\|X\\bigr\\|^2 \\Bigl(G_t - b(s_t)\\Bigr)^2\\Bigr]$\n",
    "\n",
    "这相当于一个加权的最小二乘问题,可得到最优解为：\n",
    "\n",
    "$$\n",
    "b^{*}(s_t) = \\frac{\\mathbb{E}_{a_t}\\Bigl[c(s_t,a_t)\\, G_t\\Bigr]}{\\mathbb{E}_{a_t}\\Bigl[c(s_t,a_t)\\Bigr]} = \\frac{\\mathbb{E}_{a_t}\\Bigl[\\bigl\\|\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t\\mid s_t)\\bigr\\|^2\\, G_t\\Bigr]}{\\mathbb{E}_{a_t}\\Bigl[\\bigl\\|\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t\\mid s_t)\\bigr\\|^2\\Bigr]}\n",
    "$$\n",
    "\n",
    "若对于给定状态 $s_t$ 下，$\\bigl\\|\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t\\mid s_t)\\bigr\\|^2$ 关于动作 $a_t$ 没有变化（即为常数），则\n",
    "$$b^{*}(s_t)=\\frac{\\textstyle c(s_t)\\, \\mathbb{E}_{a_t}[G_t]}{c(s_t)} =\\mathbb{E}_{a_t}[G_t]=\\mathbb{E}_{\\tau}\\left[ \\sum_{t'=t}^{T-1} r_{t'} \\,\\Big|\\, s_t \\right]=V(s_t)$$\n",
    "\n",
    "即最优 baseline 就等于状态价值函数 $V(s_t)$。\n",
    "\n",
    "2.\n",
    "\n",
    "记不用 baseline（即 $b(s_t)=0$）时的梯度估计为\n",
    "\n",
    "$$\\widehat{\\nabla_{\\theta} J}^{\\prime} = \\frac{1}{N}\\sum_{k=1}^{N} \\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\bigl(a_{t}^{k} \\mid s_{t}^{k}\\bigr)\\, G_t^k \\right]$$\n",
    "\n",
    "使用 baseline $b(s_t)$ 后的梯度估计为\n",
    "\n",
    "$$\\widehat{\\nabla_{\\theta} J}(b) = \\frac{1}{N}\\sum_{k=1}^{N} \\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\bigl(a_{t}^{k} \\mid s_{t}^{k}\\bigr)\\Bigl( G_t^k - b(s_t^k) \\Bigr) \\right]$$\n",
    "\n",
    "\n",
    "对于固定 $s_t$ 和对应 $a_t$ 的取样，记则第 $k$ 个样本对梯度估计的贡献为\n",
    "\n",
    "$$Z^k(b) = \\Delta^k \\Bigl( G_t^k - b(s_t) \\Bigr)$$\n",
    "\n",
    "\n",
    "所以其方差为\n",
    "\n",
    "$$\\operatorname{Var}\\Bigl(Z^k(b)\\Bigr) = \\mathbb{E}\\Bigl[\\bigl\\|\\Delta^k\\bigr\\|^2 \\Bigl(G_t^k - b(s_t)\\Bigr)^2\\Bigr] \\quad$$\n",
    "\n",
    "将 $b(s_t)$ 作为可调参数，用最小二乘法求解问题\n",
    "\n",
    "$$\\min_{b(s_t)} \\; \\mathbb{E}\\Bigl[\\bigl\\|\\Delta^k\\bigr\\|^2 \\Bigl(G_t^k - b(s_t)\\Bigr)^2\\Bigr]$$\n",
    "\n",
    "因此有\n",
    "\n",
    "$$\\operatorname{Var}\\Bigl(Z^k(b^*(s_t))\\Bigr) \\le \\operatorname{Var}\\Bigl(Z^k(0)\\Bigr)$$\n",
    "\n",
    "由此，对所有样本取平均后，可以证明使用最优 baseline 的梯度估计方差小于不使用 baseline 的梯度估计方差。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
